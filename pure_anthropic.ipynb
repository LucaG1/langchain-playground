{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T12:48:39.965017Z",
     "start_time": "2024-12-22T12:48:24.422061Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()"
   ],
   "id": "8e92059ef08bcccb",
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-22T12:48:53.044725Z",
     "start_time": "2024-12-22T12:48:44.736875Z"
    }
   },
   "source": [
    "import anthropic\n",
    "\n",
    "client = anthropic.Anthropic()\n",
    "\n",
    "client.models.list(limit=20)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SyncPage[ModelInfo](data=[ModelInfo(id='claude-3-5-sonnet-20241022', created_at=datetime.datetime(2024, 10, 22, 0, 0, tzinfo=datetime.timezone.utc), display_name='Claude 3.5 Sonnet (New)', type='model'), ModelInfo(id='claude-3-5-haiku-20241022', created_at=datetime.datetime(2024, 10, 22, 0, 0, tzinfo=datetime.timezone.utc), display_name='Claude 3.5 Haiku', type='model'), ModelInfo(id='claude-3-5-sonnet-20240620', created_at=datetime.datetime(2024, 6, 20, 0, 0, tzinfo=datetime.timezone.utc), display_name='Claude 3.5 Sonnet (Old)', type='model'), ModelInfo(id='claude-3-haiku-20240307', created_at=datetime.datetime(2024, 3, 7, 0, 0, tzinfo=datetime.timezone.utc), display_name='Claude 3 Haiku', type='model'), ModelInfo(id='claude-3-opus-20240229', created_at=datetime.datetime(2024, 2, 29, 0, 0, tzinfo=datetime.timezone.utc), display_name='Claude 3 Opus', type='model'), ModelInfo(id='claude-3-sonnet-20240229', created_at=datetime.datetime(2024, 2, 29, 0, 0, tzinfo=datetime.timezone.utc), display_name='Claude 3 Sonnet', type='model'), ModelInfo(id='claude-2.1', created_at=datetime.datetime(2023, 11, 21, 0, 0, tzinfo=datetime.timezone.utc), display_name='Claude 2.1', type='model'), ModelInfo(id='claude-2.0', created_at=datetime.datetime(2023, 7, 11, 0, 0, tzinfo=datetime.timezone.utc), display_name='Claude 2.0', type='model')], has_more=False, first_id='claude-3-5-sonnet-20241022', last_id='claude-2.0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T20:11:46.198923Z",
     "start_time": "2024-12-21T20:11:24.600590Z"
    }
   },
   "cell_type": "code",
   "source": [
    "our_first_message = client.messages.create(\n",
    "    model=\"claude-3-haiku-20240307\",\n",
    "    max_tokens=1000,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Hi there! Please write me a haiku about an orca whale.\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(our_first_message.content[0].text)"
   ],
   "id": "ce330d5c284ed160",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a haiku about an orca whale:\n",
      "\n",
      "Killer whale glides by,\n",
      "Powerful, majestic grace,\n",
      "Ruler of the deep.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "message = client.messages.create(\n",
    "    model=\"claude-3-haiku-20240307\",\n",
    "    max_tokens=1000,\n",
    "    temperature=0,\n",
    "    system=\"You are a world-class poet. Respond only with short poems.\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Why is the ocean salty?\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "print(message.content)"
   ],
   "id": "d8135749e9bca014"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T20:19:04.980107Z",
     "start_time": "2024-12-21T20:19:04.972873Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def translate(word, language):\n",
    "  response = client.messages.create(\n",
    "      model=\"claude-3-haiku-20240307\",\n",
    "      max_tokens=1000,\n",
    "      messages=[\n",
    "          {\"role\": \"user\", \"content\": f\"Translate the word {word} into {language}.  Only respond with the translated word, nothing else\"}\n",
    "      ]\n",
    "  )\n",
    "  return response.content[0].text"
   ],
   "id": "967af7411f10e3c9",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T20:19:53.156535Z",
     "start_time": "2024-12-21T20:19:30.296465Z"
    }
   },
   "cell_type": "code",
   "source": "translate(\"rainbow\", \"German\")",
   "id": "6beac45f6cd558fb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Regenbogen'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T20:23:57.429805Z",
     "start_time": "2024-12-21T20:22:09.476215Z"
    }
   },
   "cell_type": "code",
   "source": [
    "conversation_history = []\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"User: \")\n",
    "    \n",
    "    if user_input.lower() == \"quit\":\n",
    "        print(\"Conversation ended.\")\n",
    "        break\n",
    "    \n",
    "    conversation_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "    response = client.messages.create(\n",
    "        model=\"claude-3-haiku-20240307\",\n",
    "        messages=conversation_history,\n",
    "        max_tokens=500\n",
    "    )\n",
    "\n",
    "    assistant_response = response.content[0].text\n",
    "    print(f\"Assistant: {assistant_response}\")\n",
    "    conversation_history.append({\"role\": \"assistant\", \"content\": assistant_response})"
   ],
   "id": "a2629edd123a0826",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Getting rejected for a job can be really disappointing and discouraging. Here are some tips to help you get in the right mindset after facing a few rejections:\n",
      "\n",
      "1. Remember it's not personal: Job rejections are rarely a reflection on your worth or capabilities. There are often many qualified candidates applying for the same role, and the decision often comes down to factors outside your control.\n",
      "\n",
      "2. Reframe the experience: Instead of seeing the rejections as failures, try to view them as opportunities to learn and grow. Reflect on the feedback you received and think about how you can improve for the next application.\n",
      "\n",
      "3. Focus on what you can control: You can't control whether you get a job offer, but you can control your mindset, the effort you put into your search, and your ability to learn and grow from each experience.\n",
      "\n",
      "4. Celebrate small wins: Acknowledge and celebrate the progress you're making, whether it's landing an interview, getting positive feedback, or simply taking the steps to apply for roles.\n",
      "\n",
      "5. Maintain a balanced perspective: Rejection is a normal part of the job search process. Try not to let a few setbacks define your self-worth or make you feel like you're not good enough.\n",
      "\n",
      "6. Seek support: Talk to friends, family, or a career coach who can provide encouragement and help you stay motivated during this challenging time.\n",
      "\n",
      "The key is to stay positive, persistent, and focused on your long-term goals. With the right mindset, you can use rejection as fuel to keep pushing forward and ultimately find the right fit for your skills and aspirations.\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'messages.2: all messages must have non-empty content except for the optional final assistant message'}}",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mBadRequestError\u001B[0m                           Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 12\u001B[0m\n\u001B[0;32m      8\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m     10\u001B[0m conversation_history\u001B[38;5;241m.\u001B[39mappend({\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrole\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muser\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcontent\u001B[39m\u001B[38;5;124m\"\u001B[39m: user_input})\n\u001B[1;32m---> 12\u001B[0m response \u001B[38;5;241m=\u001B[39m client\u001B[38;5;241m.\u001B[39mmessages\u001B[38;5;241m.\u001B[39mcreate(\n\u001B[0;32m     13\u001B[0m     model\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclaude-3-haiku-20240307\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     14\u001B[0m     messages\u001B[38;5;241m=\u001B[39mconversation_history,\n\u001B[0;32m     15\u001B[0m     max_tokens\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m500\u001B[39m\n\u001B[0;32m     16\u001B[0m )\n\u001B[0;32m     18\u001B[0m assistant_response \u001B[38;5;241m=\u001B[39m response\u001B[38;5;241m.\u001B[39mcontent[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mtext\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAssistant: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00massistant_response\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\langchain-playground\\Lib\\site-packages\\anthropic\\_utils\\_utils.py:275\u001B[0m, in \u001B[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    273\u001B[0m             msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMissing required argument: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mquote(missing[\u001B[38;5;241m0\u001B[39m])\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    274\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(msg)\n\u001B[1;32m--> 275\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\langchain-playground\\Lib\\site-packages\\anthropic\\resources\\messages\\messages.py:901\u001B[0m, in \u001B[0;36mMessages.create\u001B[1;34m(self, max_tokens, messages, model, metadata, stop_sequences, stream, system, temperature, tool_choice, tools, top_k, top_p, extra_headers, extra_query, extra_body, timeout)\u001B[0m\n\u001B[0;32m    894\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m model \u001B[38;5;129;01min\u001B[39;00m DEPRECATED_MODELS:\n\u001B[0;32m    895\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m    896\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe model \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m is deprecated and will reach end-of-life on \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mDEPRECATED_MODELS[model]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mPlease migrate to a newer model. Visit https://docs.anthropic.com/en/docs/resources/model-deprecations for more information.\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    897\u001B[0m         \u001B[38;5;167;01mDeprecationWarning\u001B[39;00m,\n\u001B[0;32m    898\u001B[0m         stacklevel\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m,\n\u001B[0;32m    899\u001B[0m     )\n\u001B[1;32m--> 901\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_post(\n\u001B[0;32m    902\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/v1/messages\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    903\u001B[0m     body\u001B[38;5;241m=\u001B[39mmaybe_transform(\n\u001B[0;32m    904\u001B[0m         {\n\u001B[0;32m    905\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmax_tokens\u001B[39m\u001B[38;5;124m\"\u001B[39m: max_tokens,\n\u001B[0;32m    906\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m: messages,\n\u001B[0;32m    907\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m: model,\n\u001B[0;32m    908\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmetadata\u001B[39m\u001B[38;5;124m\"\u001B[39m: metadata,\n\u001B[0;32m    909\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstop_sequences\u001B[39m\u001B[38;5;124m\"\u001B[39m: stop_sequences,\n\u001B[0;32m    910\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstream\u001B[39m\u001B[38;5;124m\"\u001B[39m: stream,\n\u001B[0;32m    911\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msystem\u001B[39m\u001B[38;5;124m\"\u001B[39m: system,\n\u001B[0;32m    912\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtemperature\u001B[39m\u001B[38;5;124m\"\u001B[39m: temperature,\n\u001B[0;32m    913\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtool_choice\u001B[39m\u001B[38;5;124m\"\u001B[39m: tool_choice,\n\u001B[0;32m    914\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtools\u001B[39m\u001B[38;5;124m\"\u001B[39m: tools,\n\u001B[0;32m    915\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtop_k\u001B[39m\u001B[38;5;124m\"\u001B[39m: top_k,\n\u001B[0;32m    916\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtop_p\u001B[39m\u001B[38;5;124m\"\u001B[39m: top_p,\n\u001B[0;32m    917\u001B[0m         },\n\u001B[0;32m    918\u001B[0m         message_create_params\u001B[38;5;241m.\u001B[39mMessageCreateParams,\n\u001B[0;32m    919\u001B[0m     ),\n\u001B[0;32m    920\u001B[0m     options\u001B[38;5;241m=\u001B[39mmake_request_options(\n\u001B[0;32m    921\u001B[0m         extra_headers\u001B[38;5;241m=\u001B[39mextra_headers, extra_query\u001B[38;5;241m=\u001B[39mextra_query, extra_body\u001B[38;5;241m=\u001B[39mextra_body, timeout\u001B[38;5;241m=\u001B[39mtimeout\n\u001B[0;32m    922\u001B[0m     ),\n\u001B[0;32m    923\u001B[0m     cast_to\u001B[38;5;241m=\u001B[39mMessage,\n\u001B[0;32m    924\u001B[0m     stream\u001B[38;5;241m=\u001B[39mstream \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    925\u001B[0m     stream_cls\u001B[38;5;241m=\u001B[39mStream[RawMessageStreamEvent],\n\u001B[0;32m    926\u001B[0m )\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\langchain-playground\\Lib\\site-packages\\anthropic\\_base_client.py:1279\u001B[0m, in \u001B[0;36mSyncAPIClient.post\u001B[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001B[0m\n\u001B[0;32m   1265\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpost\u001B[39m(\n\u001B[0;32m   1266\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   1267\u001B[0m     path: \u001B[38;5;28mstr\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1274\u001B[0m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_StreamT] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   1275\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ResponseT \u001B[38;5;241m|\u001B[39m _StreamT:\n\u001B[0;32m   1276\u001B[0m     opts \u001B[38;5;241m=\u001B[39m FinalRequestOptions\u001B[38;5;241m.\u001B[39mconstruct(\n\u001B[0;32m   1277\u001B[0m         method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpost\u001B[39m\u001B[38;5;124m\"\u001B[39m, url\u001B[38;5;241m=\u001B[39mpath, json_data\u001B[38;5;241m=\u001B[39mbody, files\u001B[38;5;241m=\u001B[39mto_httpx_files(files), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions\n\u001B[0;32m   1278\u001B[0m     )\n\u001B[1;32m-> 1279\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(ResponseT, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrequest(cast_to, opts, stream\u001B[38;5;241m=\u001B[39mstream, stream_cls\u001B[38;5;241m=\u001B[39mstream_cls))\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\langchain-playground\\Lib\\site-packages\\anthropic\\_base_client.py:956\u001B[0m, in \u001B[0;36mSyncAPIClient.request\u001B[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001B[0m\n\u001B[0;32m    953\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    954\u001B[0m     retries_taken \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m--> 956\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_request(\n\u001B[0;32m    957\u001B[0m     cast_to\u001B[38;5;241m=\u001B[39mcast_to,\n\u001B[0;32m    958\u001B[0m     options\u001B[38;5;241m=\u001B[39moptions,\n\u001B[0;32m    959\u001B[0m     stream\u001B[38;5;241m=\u001B[39mstream,\n\u001B[0;32m    960\u001B[0m     stream_cls\u001B[38;5;241m=\u001B[39mstream_cls,\n\u001B[0;32m    961\u001B[0m     retries_taken\u001B[38;5;241m=\u001B[39mretries_taken,\n\u001B[0;32m    962\u001B[0m )\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\langchain-playground\\Lib\\site-packages\\anthropic\\_base_client.py:1060\u001B[0m, in \u001B[0;36mSyncAPIClient._request\u001B[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001B[0m\n\u001B[0;32m   1057\u001B[0m         err\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mread()\n\u001B[0;32m   1059\u001B[0m     log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRe-raising status error\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m-> 1060\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_status_error_from_response(err\u001B[38;5;241m.\u001B[39mresponse) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1062\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_process_response(\n\u001B[0;32m   1063\u001B[0m     cast_to\u001B[38;5;241m=\u001B[39mcast_to,\n\u001B[0;32m   1064\u001B[0m     options\u001B[38;5;241m=\u001B[39moptions,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1068\u001B[0m     retries_taken\u001B[38;5;241m=\u001B[39mretries_taken,\n\u001B[0;32m   1069\u001B[0m )\n",
      "\u001B[1;31mBadRequestError\u001B[0m: Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'messages.2: all messages must have non-empty content except for the optional final assistant message'}}"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T22:29:13.373681Z",
     "start_time": "2024-12-21T22:29:11.116235Z"
    }
   },
   "cell_type": "code",
   "source": [
    "stream = client.messages.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"How to lead a conversation?\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"claude-3-haiku-20240307\",\n",
    "    max_tokens=1000,\n",
    "    temperature=0,\n",
    "    stream=True,\n",
    ")\n",
    "for event in stream:\n",
    "    if event.type == \"message_start\":\n",
    "        input_tokens = event.message.usage.input_tokens\n",
    "        print(\"MESSAGE START EVENT\", flush=True)\n",
    "        print(f\"Input tokens used: {input_tokens}\", flush=True)\n",
    "        print(\"========================\")\n",
    "    elif event.type == \"content_block_delta\":\n",
    "        print(event.delta.text, flush=True, end=\"\")\n",
    "    elif event.type == \"message_delta\":\n",
    "        output_tokens = event.usage.output_tokens\n",
    "        print(\"\\n========================\", flush=True)\n",
    "        print(\"MESSAGE DELTA EVENT\", flush=True)\n",
    "        print(f\"Output tokens used: {output_tokens}\", flush=True)\n",
    "        "
   ],
   "id": "db275912a21233c4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MESSAGE START EVENT\n",
      "Input tokens used: 13\n",
      "========================\n",
      "Here are some tips for leading a conversation effectively:\n",
      "\n",
      "1. Start with open-ended questions. Ask questions that can't be answered with just a yes or no, like \"How was your weekend?\" or \"What have you been up to lately?\" This encourages the other person to share more.\n",
      "\n",
      "2. Listen actively. Pay attention to what the other person is saying and ask follow-up questions to show you're engaged. Avoid interrupting them.\n",
      "\n",
      "3. Find common ground. Look for shared interests, experiences or perspectives that you can build the conversation around.\n",
      "\n",
      "4. Share relevant information about yourself. Offer insights or anecdotes that relate to the topic, but don't dominate the conversation.\n",
      "\n",
      "5. Guide the flow. Gently steer the conversation in a direction that interests you both. You can introduce new topics or themes when the current one starts to wind down.\n",
      "\n",
      "6. Be positive and enthusiastic. Use an upbeat tone and body language to create an engaging atmosphere.\n",
      "\n",
      "7. Manage the time. Be aware of when the conversation is starting to drag or if you need to wrap it up. Politely transition to a conclusion when appropriate.\n",
      "\n",
      "The key is to balance sharing information, asking questions, and actively listening. With practice, you can become an effective conversation leader.\n",
      "========================\n",
      "MESSAGE DELTA EVENT\n",
      "Output tokens used: 275\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from anthropic import AsyncAnthropic\n",
    "\n",
    "client = AsyncAnthropic()\n",
    "\n",
    "async def streaming_with_helpers():\n",
    "    async with client.messages.stream(\n",
    "        max_tokens=1024,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Write me sonnet about orchids\",\n",
    "            }\n",
    "        ],\n",
    "        model=\"claude-3-haiku-20240307\",\n",
    "    ) as stream:\n",
    "        async for text in stream.text_stream:\n",
    "            print(text, end=\"\", flush=True)\n",
    "\n",
    "    final_message = await stream.get_final_message()\n",
    "    print(\"\\n\\nSTREAMING IS DONE.  HERE IS THE FINAL ACCUMULATED MESSAGE: \")\n",
    "    print(final_message.to_json())\n",
    "\n",
    "await streaming_with_helpers()"
   ],
   "id": "55f219a34e389b22"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Image Prompting",
   "id": "9f441c7ac88e0d5b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T12:47:27.902240Z",
     "start_time": "2024-12-22T12:47:27.889172Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import base64\n",
    "import mimetypes\n",
    "\n",
    "def create_image_message(image_path):\n",
    "    # Open the image file in \"read binary\" mode\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        # Read the contents of the image as a bytes object\n",
    "        binary_data = image_file.read()\n",
    "    \n",
    "    # Encode the binary data using Base64 encoding\n",
    "    base64_encoded_data = base64.b64encode(binary_data)\n",
    "    \n",
    "    # Decode base64_encoded_data from bytes to a string\n",
    "    base64_string = base64_encoded_data.decode('utf-8')\n",
    "    \n",
    "    # Get the MIME type of the image based on its file extension\n",
    "    mime_type, _ = mimetypes.guess_type(image_path)\n",
    "    \n",
    "    # Create the image block\n",
    "    image_block = {\n",
    "        \"type\": \"image\",\n",
    "        \"source\": {\n",
    "            \"type\": \"base64\",\n",
    "            \"media_type\": mime_type,\n",
    "            \"data\": base64_string\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    \n",
    "    return image_block"
   ],
   "id": "e4eb395f1db06f4b",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T12:49:32.272142Z",
     "start_time": "2024-12-22T12:49:07.602081Z"
    }
   },
   "cell_type": "code",
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            create_image_message(\"./IMG_5381.PNG\")\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "response = client.messages.create(\n",
    "    model=\"claude-3-haiku-20240307\",\n",
    "    max_tokens=1000,\n",
    "    messages=messages\n",
    ")\n",
    "print(response.content[0].text)"
   ],
   "id": "6fa3cb0a5be0249e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image depicts a serene and picturesque natural landscape. It shows a person standing on a rocky outcrop, overlooking a lush, forested valley with a turquoise-colored lake in the distance. The surrounding environment is characterized by towering evergreen trees, mossy rocks, and a clear blue sky. The person appears to be taking in the breathtaking view, seemingly enjoying the tranquility and beauty of this wilderness setting.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T12:52:38.125036Z",
     "start_time": "2024-12-22T12:52:07.634842Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import base64\n",
    "import httpx\n",
    "\n",
    "image_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Church_of_light.jpg/1599px-Church_of_light.jpg\"\n",
    "image_media_type = \"image/jpeg\"\n",
    "image_data = base64.b64encode(httpx.get(image_url).content).decode(\"utf-8\")\n",
    "\n",
    "messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image\",\n",
    "                    \"source\": {\n",
    "                        \"type\": \"base64\",\n",
    "                        \"media_type\": image_media_type,\n",
    "                        \"data\": image_data,\n",
    "                    },\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Describe this image.\"\n",
    "                }\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "\n",
    "\n",
    "response = client.messages.create(\n",
    "    model=\"claude-3-haiku-20240307\",\n",
    "    max_tokens=1000,\n",
    "    messages=messages\n",
    ")\n",
    "print(response.content[0].text)\n",
    "\n"
   ],
   "id": "917c91ccd2c97ede",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image depicts a serene and picturesque winter landscape in Iceland. In the foreground, there is a small church with a distinctive red-roofed steeple, surrounded by a snowy, rocky landscape. In the sky above, the stunning natural phenomenon of the Northern Lights, or aurora borealis, can be seen in vibrant green and white shimmering patterns. The church stands alone against the backdrop of the rugged mountainous terrain, creating a striking contrast between the man-made structure and the untamed natural environment. The overall scene conveys a sense of remote, wintry beauty and the awe-inspiring power of nature.\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import base64\n",
    "import mimetypes\n",
    "\n",
    "research_paper_pages = [\n",
    "    \"./images/research_paper/page1.png\",\n",
    "    \"./images/research_paper/page2.png\",\n",
    "    \"./images/research_paper/page3.png\",\n",
    "    \"./images/research_paper/page4.png\",\n",
    "    \"./images/research_paper/page5.png\"\n",
    "    ]\n",
    "\n",
    "def create_image_message(image_path):\n",
    "    # Open the image file in \"read binary\" mode\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        # Read the contents of the image as a bytes object\n",
    "        binary_data = image_file.read()\n",
    "    \n",
    "    # Encode the binary data using Base64 encoding\n",
    "    base64_encoded_data = base64.b64encode(binary_data)\n",
    "    \n",
    "    # Decode base64_encoded_data from bytes to a string\n",
    "    base64_string = base64_encoded_data.decode('utf-8')\n",
    "    \n",
    "    # Get the MIME type of the image based on its file extension\n",
    "    mime_type, _ = mimetypes.guess_type(image_path)\n",
    "    \n",
    "    # Create the image block\n",
    "    image_block = {\n",
    "        \"type\": \"image\",\n",
    "        \"source\": {\n",
    "            \"type\": \"base64\",\n",
    "            \"media_type\": mime_type,\n",
    "            \"data\": base64_string\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    \n",
    "    return image_block\n",
    "\n",
    "def transcribe_single_page(page_url):\n",
    "    messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            create_image_message(page_url),\n",
    "            {\"type\": \"text\", \"text\": \"transcribe the text from this page of a research paper as accurately as possible.\"}\n",
    "        ]\n",
    "    }\n",
    "    ]\n",
    "\n",
    "    response = client.messages.create(\n",
    "        model=\"claude-3-5-sonnet-20240620\",\n",
    "        max_tokens=5000,\n",
    "        messages=messages\n",
    "    )\n",
    "    return response.content[0].text\n",
    "\n",
    "def summarize_paper(pages):\n",
    "    complete_paper_text = \"\"\n",
    "    for page in pages:\n",
    "        print(\"transcribing page \", page)\n",
    "        transribed_text = transcribe_single_page(page)\n",
    "        print(transribed_text[:200])\n",
    "        complete_paper_text += transribed_text\n",
    "    response = client.messages.create(\n",
    "        model=\"claude-3-5-sonnet-20240620\",\n",
    "        max_tokens=5000,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"This is the transcribed contents of a research paper <paper>{complete_paper_text}</paper>.  Please summarize this paper for a non-research audience in at least 3 paragraphs.  Make to sure explain any abbreviations or technical jargon, and use analogies when possible\"\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    print(response.content[0].text)\n"
   ],
   "id": "290842cb3915cce9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
